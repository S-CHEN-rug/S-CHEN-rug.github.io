
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Shuyi Chen | Voice Technology</title>
  <script>
    function toggleLanguage() {
      document.body.classList.toggle('lang-zh');
    }
  </script>
</head>
<body>
  <header style="overflow: auto;">
    
    <h1>Shuyi Chen / 陈姝伊</h1>
    <p>MSc Voice Technology | University of Groningen</p>
    <button onclick="toggleLanguage()">🌐 Switch Language / 切换语言</button>
  </header>

  <nav>
    <a href="#about">About / 关于</a>
    <a href="#project">Project / 项目</a>
    <a href="#contact">Contact / 联系</a>
  </nav>

  <main>
    <section id="about">
      <h2>About Me / 关于我</h2>
      <p class="en">I am a Master's student in Voice Technology at the University of Groningen, focusing on expressive and controllable Text-to-Speech systems.</p>
      <p class="zh">我是在格罗宁根大学攻读语音技术硕士的学生，研究方向为可控与具表现力的语音合成系统。</p>
    </section>

    <section id="project">
      <h2>Current Project / 当前项目</h2>
      <h3>Enhancing Surprise Perception in TTS through Keyword-Level Prosody Control</h3>
      <p class="en">This project proposes a prosody control framework to enhance surprise in synthesized speech using FastSpeech2 and HiFi-GAN.</p>
      <p class="zh">本项目提出一种节奏控制框架，通过在关键词处调节音高与能量增强语音合成中的“惊讶”感知。</p>
      <p><a href="your-thesis-pdf-link.pdf" target="_blank">📄 Download Thesis / 下载论文 (PDF)</a></p>
      <p><a href="https://github.com/S-CHEN-rug" target="_blank">🔗 GitHub Repository / 项目代码</a></p>
    </section>

    <section id="contact">
      <h2>Contact / 联系方式</h2>
      <p>Email: <a href="mailto:s.chen@student.rug.nl">s.chen.66@student.rug.nl</a></p>
      <p>GitHub: <a href="https://github.com/S-CHEN-rug" target="_blank">S-CHEN-rug</a></p>
    </section>

<div style="text-align:right; margin-bottom:1em;">
  <label for="langToggle">🌐 Language:</label>
  <select id="langToggle" onchange="toggleLang()">
    <option value="en">English</option>
    <option value="zh">中文</option>
  </select>
</div>
<script>
  function toggleLang() {
    var lang = document.getElementById("langToggle").value;
    document.querySelectorAll('.en').forEach(el => el.style.display = lang === 'en' ? 'block' : 'none');
    document.querySelectorAll('.zh').forEach(el => el.style.display = lang === 'zh' ? 'block' : 'none');
  }
  window.onload = toggleLang;
</script>

<div style="text-align:right; margin-bottom:1em;">
  <label for="langToggle">🌐 Language:</label>
  <select id="langToggle" onchange="toggleLang()">
    <option value="en">English</option>
    <option value="zh">中文</option>
  </select>
</div>
<script>
  function toggleLang() {
    var lang = document.getElementById("langToggle").value;
    document.querySelectorAll('.en').forEach(el => el.style.display = lang === 'en' ? 'block' : 'none');
    document.querySelectorAll('.zh').forEach(el => el.style.display = lang === 'zh' ? 'block' : 'none');
  }
  window.onload = toggleLang;
</script>

<section id="overview">
  <h2>Overview / 项目概述</h2>
  <div class="en">
    <p>This project investigates whether keyword-level pitch and energy control can enhance the perception of the emotion "surprise" in neural TTS systems.
    A multilingual FastSpeech2-based TTS system is built and evaluated across two languages (Mandarin and English).</p>
  </div>
  <div class="zh">
    <p>本项目探索是否可以通过对关键词的音高和能量控制来增强神经 TTS 系统中“惊讶”情绪的感知效果。
    实验系统基于 FastSpeech2 模型，跨语言（中文和英文）进行构建与评估。</p>
  </div>
</section>

<section id="methodology">
  <h2>Methodology / 方法设计</h2>
  <div class="en">
    <p>The study uses keyword pitch/energy scaling during inference without modifying the core architecture of FastSpeech2.
    Surprise keywords were extracted using a GPT-based API, with prosody control applied via the variance adaptor.
    In addition, several implementation improvements were made:</p>
    <ul>
      <li>Emotion keyword detection and tagging</li>
      <li>Emotion type and intensity parameterization</li>
      <li>Smooth transition at keyword regions</li>
      <li>Batch structure updated to support keyword info</li>
      <li>Integration of smoothing in synthesis</li>
      <li>1D convolution smoothing on mel-spectrogram</li>
      <li>Integration of keyword smoothing in synth_samples</li>
    </ul>
  </div>
  <div class="zh">
    <p>本研究在不更改 FastSpeech2 模型主结构的前提下，在推理阶段对关键词进行音高与能量增强控制。
    惊讶关键词由 GPT 接口识别，并通过 variance adaptor 实现韵律控制。同时，本研究进行了以下改进：</p>
    <ul>
      <li>情绪关键词识别与标注</li>
      <li>情绪类别与强度参数化</li>
      <li>关键词区域的过渡平滑控制</li>
      <li>批次结构更新以支持关键词标注</li>
      <li>在合成阶段集成平滑控制</li>
      <li>对梅尔谱图进行一维卷积平滑</li>
      <li>在 synth_samples 中集成关键词平滑信息</li>
    </ul>
  </div>
</section>

<section id="results">
  <h2>Results / 实验结果</h2>
  <div class="en">
    <p>Results from both forced-choice and scalar rating tests show that keyword-level control increases surprise perception significantly compared to baseline synthesis.
    Cross-linguistic generalizability is demonstrated, though language-specific tuning is needed.</p>
  </div>
  <div class="zh">
    <p>强制选择与连续评分测试均表明，关键词控制显著增强了“惊讶”情绪的感知效果。
    该方法在中英文中均显示良好泛化性，但需进行语言特定参数微调。</p>
  </div>
</section>

<section id="demo">
  <h2>Audio Demonstration Table / 音频演示表格</h2>
  <table border="1" cellspacing="0" cellpadding="10" style="border-collapse: collapse; width: 100%; text-align: center;">
    <thead style="background-color: #990000; color: white;">
      <tr>
        <th>Example / 类型</th>
        <th>Baseline</th>
        <th>Mild</th>
        <th>Moderate</th>
        <th>Strong</th>
      </tr>
    </thead>
    <tbody>
      <tr><td>中文男声 - 例句1</td>
        <td><audio controls src="audio/zh_m_01_baseline.mp3"></audio></td>
        <td><audio controls src="audio/zh_m_01_mild.mp3"></audio></td>
        <td><audio controls src="audio/zh_m_01_moderate.mp3"></audio></td>
        <td><audio controls src="audio/zh_m_01_strong.mp3"></audio></td>
      </tr>
      <tr><td>中文男声 - 例句2</td>
        <td><audio controls src="audio/zh_m_02_baseline.mp3"></audio></td>
        <td><audio controls src="audio/zh_m_02_mild.mp3"></audio></td>
        <td><audio controls src="audio/zh_m_02_moderate.mp3"></audio></td>
        <td><audio controls src="audio/zh_m_02_strong.mp3"></audio></td>
      </tr>
      <tr><td>中文女声 - 例句1</td>
        <td><audio controls src="audio/zh_f_01_baseline.mp3"></audio></td>
        <td><audio controls src="audio/zh_f_01_mild.mp3"></audio></td>
        <td><audio controls src="audio/zh_f_01_moderate.mp3"></audio></td>
        <td><audio controls src="audio/zh_f_01_strong.mp3"></audio></td>
      </tr>
      <tr><td>中文女声 - 例句2</td>
        <td><audio controls src="audio/zh_f_02_baseline.mp3"></audio></td>
        <td><audio controls src="audio/zh_f_02_mild.mp3"></audio></td>
        <td><audio controls src="audio/zh_f_02_moderate.mp3"></audio></td>
        <td><audio controls src="audio/zh_f_02_strong.mp3"></audio></td>
      </tr>
      <tr><td>English Male - Sentence 1</td>
        <td><audio controls src="audio/en_m_01_baseline.mp3"></audio></td>
        <td><audio controls src="audio/en_m_01_mild.mp3"></audio></td>
        <td><audio controls src="audio/en_m_01_moderate.mp3"></audio></td>
        <td><audio controls src="audio/en_m_01_strong.mp3"></audio></td>
      </tr>
      <tr><td>English Male - Sentence 2</td>
        <td><audio controls src="audio/en_m_02_baseline.mp3"></audio></td>
        <td><audio controls src="audio/en_m_02_mild.mp3"></audio></td>
        <td><audio controls src="audio/en_m_02_moderate.mp3"></audio></td>
        <td><audio controls src="audio/en_m_02_strong.mp3"></audio></td>
      </tr>
      <tr><td>English Female - Sentence 1</td>
        <td><audio controls src="audio/en_f_01_baseline.mp3"></audio></td>
        <td><audio controls src="audio/en_f_01_mild.mp3"></audio></td>
        <td><audio controls src="audio/en_f_01_moderate.mp3"></audio></td>
        <td><audio controls src="audio/en_f_01_strong.mp3"></audio></td>
      </tr>
      <tr><td>English Female - Sentence 2</td>
        <td><audio controls src="audio/en_f_02_baseline.mp3"></audio></td>
        <td><audio controls src="audio/en_f_02_mild.mp3"></audio></td>
        <td><audio controls src="audio/en_f_02_moderate.mp3"></audio></td>
        <td><audio controls src="audio/en_f_02_strong.mp3"></audio></td>
      </tr>
    </tbody>
  </table>
</section>

</main>

  <footer>
    <p>&copy; 2025 Shuyi Chen. All rights reserved.</p>
  </footer>

  <style>
    body { font-family: Arial, sans-serif; margin: 0; padding: 0; background: white; color: black; }
    header { background-color: #990000; color: white; padding: 2rem 1rem; text-align: center; }
    nav { background: #000; display: flex; justify-content: center; gap: 1rem; padding: 0.5rem; }
    nav a { color: white; text-decoration: none; padding: 0.5rem; }
    nav a:hover { background: #990000; }
    main { padding: 2rem; max-width: 900px; margin: auto; }
    section { margin-bottom: 2rem; }
    footer { background: #f1f1f1; text-align: center; padding: 1rem; font-size: 0.9rem; }
    .zh { display: none; }
    .lang-zh .en { display: none; }
    .lang-zh .zh { display: block; }
    button { margin-top: 1rem; padding: 0.5rem 1rem; background: #000; color: #fff; border: none; cursor: pointer; }
    button:hover { background: #990000; }
    
  </style>
</body>
</html>
